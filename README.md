 🧠 TensorFlow MNIST 手寫數字分類

本專案使用 TensorFlow  建立卷積神經網路（CNN），用來訓練與辨識 MNIST 手寫數字影像，並儲存模型後載入做推論。

---

## 🧩 模型架構

- Conv2D (32 filters, 3x3) + ReLU + MaxPooling
- Conv2D (64 filters, 3x3) + ReLU + MaxPooling
- Flatten 展平成向量
- Dense(128) + ReLU
- Dense(10) + Softmax（輸出 0~9 機率）

---

## 📈 訓練資訊

- 訓練資料：MNIST 60,000 張手寫數字圖像
- 驗證準確率：約 97.7%
- 測試集準確率：約 97.7%
- 儲存模型為：`model_cnn_tf.h5`

---

## 🧪 推論流程

1. 執行 `mnist_cnn_tf_inference.py`
2. 自動載入測試集中的某一張圖片
3. 顯示模型預測結果與正確答案

你可以修改 `index = 0` 來測不同的圖片。

---

## 📂 檔案說明

| 檔案 | 用途 |
|------|------|
| `mnist_cnn_tf.py` | 建立並訓練 CNN 模型，儲存為 `.h5` |
| `mnist_cnn_tf_inference.py` | 載入模型並預測單張 MNIST 測試圖片 |
| `model_cnn_tf.h5` | 訓練好的模型參數，可供推論用 |
| `README.md` | 專案說明文件 |



🎵 白話例子：教AI辨認音樂曲風
假設你正在訓練一個 AI 來判斷音樂是什麼曲風（搖滾、爵士、嘻哈、古典…）

🪵 1️⃣ 收集資料 → 就像你蒐集了很多音樂片段
每段音樂是 x_train（輸入）

每段音樂的風格是 y_train（標籤）

就像你給 AI 一堆音樂，然後告訴它「這是爵士」、「這是搖滾」。

✂️ 2️⃣ 前處理音樂 → 把音樂轉成 AI 能聽懂的樣子
音樂不能直接丟進去，要先轉成「聲音的圖」（像頻譜圖 spectrogram）

然後正規化（音量範圍統一）

就像你先把不同歌手的音量統一好，變成一張張圖片讓 AI 看懂。

📦 3️⃣ 分批給 AI 聽 → batch_size = 32
一次給 AI 聽 32 首歌來學習

不會一口氣塞進去 5000 首，AI 吃不下

像是在上音樂課，一次上 32 首歌的內容，聽完才繼續下一批。

🧠 4️⃣ 開始學習 → model.fit()
AI 猜「這應該是搖滾吧？」

如果猜錯（其實是爵士），就會反省、微調「聽歌規則」

有點像音樂老師說：「你搞錯了，這段有薩克斯風，是爵士不是搖滾喔」，然後 AI 調整。

🔁 5️⃣ 多聽幾輪 → epochs = 10
聽一次不夠準，再讓 AI 聽 10 遍

每遍都會修正它的錯誤

就像學生反覆聽歌學習，越聽越準。

💾 6️⃣ 存下來學會的規則 → .h5 模型檔
你把這個「辨認音樂風格的能力」存成一個模型

下次可以直接用，不用重聽一次

就像你存了一本「音樂風格辨識手冊」，誰都可以用它來分類音樂。

🔍 7️⃣ 預測新歌是什麼風格 → model.predict()
丟一首沒見過的新歌

AI 馬上告訴你：「這是嘻哈」

它靠以前學到的知識推斷，就像音樂達人一聽就知道是什麼類型。